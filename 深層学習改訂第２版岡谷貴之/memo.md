# 2章　ネットワークの基本構造
* 問題の定式化
    * 回帰
    * 2値分類
    * 多クラス分類
    * マルチラベル分類...複数のラベル(メガネ，ひげ...etc)についてその有無を予測する2値分類に帰着
    * 順序回帰...5段階評価(とても悪い，悪い，普通，良い，とても良い)を予測するなど
# 3章　確率的勾配降下法
## 3.1確率的勾配降下法
* バッチ学習...訓練データを全て利用する．それぞれのデータの損失関数の和をとって，パラメータを更新する方法
    * 局所解に陥りやすい
* 確率的勾配降下法...ランダムに選ばれた訓練データ１つの損失を使ってパラメータを更新する方法
    * 時間がかかる
* ミニバッチ学習...訓練データをまとまりに分けて，まとまりごとにパラメータを更新する方法
    * エポック...ミニバッチが全て使われた状態を１エポック
## 3.2汎化性能と過学習
* バイアス・分散トレードオフ...誤差が，パラメータ数が小さい時に大きく，また大きいときも大きくなること
* バリデーションデータ（検証データ）...学習時にテストデータの代わりとするもの．バリデーションデータは，早期終了の判断や学習率やモメンタムなどのハイパラの調整に使われる．
## 3.3正則化
* 正則化(regularization)...パラネータに一定の制約を課してモデルの自由度を下げて，過学習を回避する
    * 重みの減衰...L2正則化 $$ E_t(w) = \frac{1}{N_t} \sum_{n\in D_t}E_n(w)+\frac{\lambda}{2}||w||^2$$ $\lambda$は正則化の効果の強さを制御するパラ．重みが大きくなるほど更新の速さが遅くなる．
    * ドロップアウト...ネットワーク内のユニットを学習時のみランダムに選択して削除する方法．
## 3.4学習率の選定
* 学習率を大きくしすぎると，学習が収束せず，小さくしすぎると収束するまでの更新回数が増え，また化学週に陥りやすい．
## 3.5　SGDの改良
重みの更新を$w_{t+1}=w_t- \epsilon \nabla E_t$と書く代わりに，更新料$\Delta w_t \equiv w_{t+1} - w_t$を使って表記する．損失関数の勾配を$g_t \equiv \nabla E_t$,そのi成分を$g_{t,i}$とかくと，$\Delta w_t$のi成分$\Delta w_{t,i}$は$$\Delta w_{t,i} = -\epsilon g_{t,i}$$とかける．
<br>
<br>

### 3.5.1 AdaGrad 
$$ \Delta w_{t,i} = - \frac{\epsilon}{\sqrt{\sum_{t'=1}^{t}g_{t',i}^2+\epsilon}}g_{t,i}$$
$\sum_{t'=1}^{t}g_{t',i}^2+\epsilon$は，学習開始から現在に至るまでの全更新について勾配の二乗を計算する．これまで大きく更新が行われた成分iを控えめに更新し，逆にそうでない成分iを相対的に大きくする更新
    * 学習が進むほど更新幅が小さくなり，いずれ０になる

<br>
<br>

### 3.5.2 RMSProp 
$$ \Delta w_{t,i} = - \frac{\epsilon}{\sqrt{<g_{i}^2>_t+\epsilon}}g_{t,i} $$ 
$$  <g_{i}^2>_t = \gamma <g_{i}^2>_{t-1}+(1-\gamma)g_{t,i}^2$$ 
時間方向の移動平均をとって更新幅を決定する方法．

### 3.5.3 SAM(sharpness aware minimization)
$$ min_w max_{||\epsilon||_{p}\leq \rho} E(w+\epsilon)+\lambda||w||_2^2$$
E()は交差エントロピーなどの基本的な損失．点w周りの一定範囲の点$w+\epsilon(||\epsilon||_{p}\leq \rho)$を考え，それらに対する損失の最大値を最小化することで，平坦な極小値wを見つける．

## 3.6 層出力の正規化
* バッチ正規化...各層の出力に対して，正規化を行う．平均や分散を求めるには，ミニバッチ内のサンプルから計算される．ミニバッチないの各ユニットごとの平均，分散をそれぞれ計算する．これらの平均，分散はあらかじめ推論させておく．バッチ正規化をおこなった後，活性化関数に入力させる．
* レイヤー正規化...各層の全ユニットに対して，出力の平均や分散を計算して正規化を行う．

## 3.7 重みの初期化
* 1つの確率分布p(w)を指定し，p(w)からランダムに抽出した値を重み$w_{ji}$にセットする．
    * p(w)には正規分布$N(0,\sigma^2)$や一様分布$U(-a,a)$が選ばれる．
* Xavier法，活性化関数にReLUを使う場合は，Kamming法など

# 4章　誤差逆伝播法
中間層lの入力と出力の関係式は以下の通り．
$$ z_{j}^{(l)} = f(u_j^{(l)}) = f(\sum_i w_{ji}^{(l)}z_i^{(l-1)})  \tag{1} $$
$$ \frac{\partial E_n}{\partial w_{ji}^{(l)}} = \frac{\partial E_n}{\partial u_{j}^{(l)}} \frac{\partial u_{j}^{(l)}}{\partial w_{ji}^{(l)}} \tag{2} $$
$$\delta_j^{(l)} \equiv \frac{\partial E_n}{\partial u_j^{(l)}}  \tag{3} $$
$$\delta_j^{(l)} = \sum_{k}\delta_k^{(l+1)}(w_{kj}^{(l+1)}f'(u_j^{(l)}) \tag{4} $$
$$ \frac{\partial E_n}{\partial w_{ji}^{(l)}} = \delta_j^{(l)} z_{j}^{(l-1)} \tag{5}$$
入力:訓練サンプル$x_n$および目標出力$d_n$のペア1つ
1. $z^{(1)}=x_n$とし，各層l(=2,...,L)のユニット入出力$u^{(l)}$および$z^{(l)}$を順に計算する．
2. 出力層でのデルタ$\delta_j^{(L)}$を求める．
3. 中間層l(=L-1,L-2,...,2)での$\delta_j^{(l)}$をこの順に式(4)に従って計算する．
4. 各層l(=2,..,L)のパラメータ$w_{ji}^{(l)}$に関する微分を式(5)に従って計算する．

## 4.4 自動微分
* 微分可能...演算機構の上流（つまり入力に近い側）に，ニューラルネットワークなどの学習すべきパラメータを含む構造がある場合には，その演算機構は勾配を逆伝播できる必要がある．そのような演算機構のことを微分可能であるという．

## 4.5 勾配消失問題
* 勾配消失問題...式(4)のように，各層の重みが大きいと，勾配は各層を伝播するうちに急速に大きくなり（発散し），また逆に重みが小さいと，急速に消失し0になる．
    * ReLUなどの活性化関数，バッチ正規化，などで解決させる

## 4.6 残差接続(residual connection)
* 残差接続...1つあるいは複数の層を迂回する近道を設け，そこを通る信号を，迂回せず伝播してきた信号に加算させることで，合流させる構造．
    * 残差ブロック...スキップ接続の直前から合流後のひとまとまり
$$ z' = y+z=g(z)+z $$
$g(z)=z'-z$となり，迂回された層が表すg(z)は，このブロックの出力z'と入力z間の残差を表現する．つまり，迂回された層は，それへの入力zに対する修正量を予測する役割を持つ． 
* 逆伝播で，g()の部分で勾配が消失しても，スキップ接続が上の層からの勾配情報を下の層に確実に伝えることができる．
* 残差ブロック１つを削除したり，順番を入れ替えたりしても，性能に影響しないことが知られている．
    * 確率的深度変動法(stochastic depth)...学習時にランダムに残差ブロックを選んで削除し，学習をよりうまく進める方法


# 9章　説明と可視化

## 9.1 初めに
NNの計算の中身を説明することは難しい．説明の対象となるのは主に以下の２つ．
* 推論の過程...何をどのように計算したのか
* 学習の内容...学習を通してNNが何を獲得したのか

説明に有効な手段は以下の２つ．
* 可視化(visualization)
    * 入力のどの部分が出力に影響しているのかを定量的に示す
* 事例
    * 説明したい推論の結果に対して，同じような結果になるサンプルを集めて比較することで，なぜその推論になったかを読み取る
    * 影響関数(influence function)
        * NNの下した推論が，訓練データのどのサンプルからどれだけの影響を受けたのかを知るのに有効
    * 反事実現実ZZ(counterfactual)
        * 直感に反する推論が下された際に，望む結果を得るにはどういうサンプルが必要かをした調べること．
